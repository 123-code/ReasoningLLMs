{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bbe84d0-6dea-4b6f-adbf-4df6bd324267",
   "metadata": {},
   "source": [
    "### Reinforcement learning & LLMs:\n",
    "#### Usando REINFORCE para aprender los parametros de un modelo que maximiza la recompensa al resolver problemas matemáticos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8386cc61-040a-4aec-9a97-d11974925855",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e543a-69ce-4253-b19a-5924e9aeacd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef335d7a-5c94-4341-a1ed-8a3e2b011332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alf/Downloads/[/Users/alf/miniconda_1]/envs/autoagents/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/alf/Downloads/[/Users/alf/miniconda_1]/envs/autoagents/lib/python3.11/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from math_verify import parse, verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb24b4d2-9a70-4370-a3c3-8ca1d9354945",
   "metadata": {},
   "source": [
    "#### El modelo que usaremos como policy network es gemma-2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8005bd74-fd82-4fd5-bf1f-2fdfd13d1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-1.1-2b-it\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d49633a-bc72-4625-8bd1-1d52239ab55d",
   "metadata": {},
   "source": [
    "#### Policy es la clase que usamos para representar a nuestro lm como la red que maximiza la recompensa, al cambiar sus parametros\n",
    "\n",
    "#### * para seleccionar la acción , hacemos un sampling desde la distribución de tokens\n",
    "#### * calculamos log pobability de ese token, esto nos permite tener un rango más grande y reduce costos computacionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b830d5b9-3bba-4363-a101-e1f3a7b45f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    #gamma es el hiperparametro que usamos psra descontar recompensas futuras \n",
    "    def __init__(self,pretrained_model,gamma = 0.99):\n",
    "        super(Policy,self).__init__()\n",
    "        self.model = pretrained_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.gamma = gamma\n",
    "        # guardamos los log probabilities de cada accion\n",
    "        self.policy_history = []\n",
    "        #guardamos las recompensas de cada episodio\n",
    "        self.reward_episode = []\n",
    "    def forward(self,input_ids):\n",
    "        output = self.model(input_ids)\n",
    "        logits = outputs.logits([:,-1,:])\n",
    "        probs = nn.Softmax(dim=-1)(logits)\n",
    "        #retorna un tensor del tamano vocab size, con las probabilidades, pasado por un sofmax\n",
    "        return probs\n",
    "    def select_action(policy,input_ids):\n",
    "        probs = policy(input_ids)\n",
    "        #convertimos a los tokens en una distribucion y hacemos sampling\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample()\n",
    "        #calculamos log_probs\n",
    "        log_prob = dist.log_prob(action)\n",
    "        policy.policy_history.append(log_prob)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094a289-cc22-4f76-ae38-a8508bfa6fcc",
   "metadata": {},
   "source": [
    "### Ajustando los parámetros del modelo\n",
    "#### * damos prioridad a recompensas mas cercanas \n",
    "#### * para clacular loss: \n",
    "#### - asignamos recompensas a cada token, todos son cero excepto el ultimo (la respuesta)\n",
    "#### - ajustamos las recompensas a discounted_rewards[] usando gamma. esto da a cada token generado \"credito\" por la respuesta\n",
    "#### - loss se calcula como -= log_prob *  recompensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f46f23a-f730-4eeb-ae43-a8c7d0202052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(policy,optimizer):\n",
    "    discounted_rewards = []\n",
    "    running_reward = 0\n",
    "    #recorrer la lista desde el final\n",
    "    for r in policy.reward_episode[::-1]:\n",
    "        running_reward = r + policy.gamma * running_reward\n",
    "        discounted_rewards.insert(0,running_reward)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards,dtype=torch.float32)\n",
    "    if discounted_rewards.std() > 0:\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
    "    loss = 0\n",
    "    #multiplicamos la probabilidad del token por su recompensa\n",
    "    for log_prob,reward in zip(policy.policy_history,discounted_rewards):\n",
    "        loss -= log_prob * reward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    policy.policy_history = []\n",
    "    policy.reward_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4258b-cf85-4cf2-8d51-99f672df12bc",
   "metadata": {},
   "source": [
    "### Por que se usa el negativo en loss?\n",
    "#### Log probabilities son negativas, la multiplicacion da una respuesta negativa y necesitamos aumentar la probabilidad de buenas acciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f1d7a-5205-42be-9e53-64945399e623",
   "metadata": {},
   "source": [
    "#### tratamos al modelo como un policy network, que genera acciones(tokens)\n",
    "#### Estado: el prompt\n",
    "#### Accion: cada unpo de los tokens generados \n",
    "#### Recompensa: 1 si la respuesta es correcta, 0.5 por un buen razonamiento, 0 de otro modo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f372dc1-6a5a-44bd-bdd7-67fcf69186fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    #gamma es el hiperparametro que usamos psra descontar recompensas futuras \n",
    "    def __init__(self,pretrained_model,gamma = 0.99):\n",
    "        super(Policy,self).__init__()\n",
    "        self.model = pretrained_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.gamma = gamma\n",
    "        # guardamos los log probabilities de cada accion\n",
    "        self.policy_history = []\n",
    "        #guardamos las recompensas de cada episodio\n",
    "        self.reward_episode = []\n",
    "    def forward(self,input_ids):\n",
    "        output = self.model(input_ids)\n",
    "        logits = outputs.logits([:,-1,:])\n",
    "        probs = nn.Softmax(dim=-1)(logits)\n",
    "        #retorna un tensor del tamano vocab size, con las probabilidades, pasado por un sofmax\n",
    "        return probs\n",
    "    def select_action(policy,input_ids):\n",
    "        probs = policy(input_ids)\n",
    "        #convertimos a los tokens en una distribucion y hacemos sampling\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample()\n",
    "        #calculamos log_probs\n",
    "        log_prob = dist.log_prob(action)\n",
    "        policy.policy_history.append(log_prob)\n",
    "        return action\n",
    "policy = Policy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee871063-a9df-44ef-ad57-e0489d2f6fe9",
   "metadata": {},
   "source": [
    "### Entrenamiento:\n",
    "#### usamos Adam para actualizar parametros de gemma-2b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec5838-f166-4100-88c2-e6a513bdce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reasoning(policy,episodes = 10,max_length=5):\n",
    "    optimizer = torch.optim.Adam(policy.parameters(),lr=0.0001)\n",
    "    for episode in range(episode):\n",
    "        prompt = \"what is 3 + 5\"\n",
    "        input_ids = tokenizer(prompt,return_tensors = \"pt\").input_ids\n",
    "        generated = input_ids.clone()\n",
    "        for x in range(max_length):\n",
    "            action = select_action(policy,generated)##$\n",
    "            generated = torch.cat([generated,action.unsqueeze(0)],dim=1)\n",
    "            policy.reward_episode.append(0)\n",
    "        output_text = tokenizer.decode(generated[0],skip_special_tokens=True)\n",
    "        print(f\"Episode {episode}: Generated: {output_text}\")\n",
    "        reward = 1 if \"8\" in putput_text else 0\n",
    "        policy.reward_episode[-1] = reward\n",
    "        print(f\"Reward: {reward}\")\n",
    "        update_policy(policy,optimizer)\n",
    "        \n",
    "\n",
    "train_reasoning(policy,episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fdc0dae-88e6-4c8d-868f-f6f3e736eaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_reward():\n",
    "    ## todo get answer from dataset\n",
    "    gold = parse(\"10x - 15\")\n",
    "    answer = parse(\"3*(2x - 5) + 4x\") \n",
    "    result = verify(gold,answer)\n",
    "    return result\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c03a1e5-9355-4fb2-8b26-756292f973b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
