{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bbe84d0-6dea-4b6f-adbf-4df6bd324267",
   "metadata": {},
   "source": [
    "### Reinforcement learning & LLMs:\n",
    "#### Usando REINFORCE para aprender los parametros de gemma-2b que maximizan la recompensa acummulada al resolver problemas matemÃ¡ticos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8386cc61-040a-4aec-9a97-d11974925855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abdd495c-b576-43f4-8737-58dd696b78fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ecadcda-a942-497b-b6d9-ae889fea4154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q math-verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcebc349-4bbe-43ba-ac46-be7f00ba16a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef335d7a-5c94-4341-a1ed-8a3e2b011332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alf/Downloads/[/Users/alf/miniconda_1]/envs/autoagents/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/alf/Downloads/[/Users/alf/miniconda_1]/envs/autoagents/lib/python3.11/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from math_verify import parse, verify\n",
    "import jsonlines\n",
    "from datasets import Dataset,DatasetDict\n",
    "import os \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5407c4b-55a6-4662-83c6-a2c90a58fc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alf/Downloads/[/Users/alf/miniconda_1]/envs/autoagents/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/alf/Downloads/[/Users/alf/miniconda_1]/envs/autoagents/lib/python3.11/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "2025-03-28 17:31:11.256225: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#cargar modelo de embedding [ara calcular recompensas:\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model_embedding = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "reference_text = \"To find the next term, list the sequence. Calculate differences. Analyze them. Compute second differences. Analyze pattern. Predict next difference. Calculate next term. Answer.\"\n",
    "reference_embedding = model_embedding.encode(reference_text, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb24b4d2-9a70-4370-a3c3-8ca1d9354945",
   "metadata": {},
   "source": [
    "#### El modelo que usaremos como policy network es gemma-2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8005bd74-fd82-4fd5-bf1f-2fdfd13d1744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a754a72451b544c2b9396a81b96152c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): GemmaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-1.1-2b-it\", torch_dtype=torch.bfloat16)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf65aaf5-1a3b-44e1-b58a-28562755e4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602a8772aecd489a80a149b281392e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 218111 total examples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8650fe2b1444bb9f45a6a4e44ec313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/219 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def custom_generator(filepath):\n",
    "  try:\n",
    "    with open(filepath,'r',encoding='utf-8') as f:\n",
    "      while True:\n",
    "        question_line = f.readline()\n",
    "        answer_line = f.readline()\n",
    "        if not question_line or not answer_line:\n",
    "          break\n",
    "        question = question_line.strip()\n",
    "        answer = answer_line.strip()\n",
    "\n",
    "        if question and answer:\n",
    "\n",
    "          yield {'question':question,'answer':answer}\n",
    "\n",
    "        elif question or answer:\n",
    "\n",
    "          print(f\"Warning: Skipping potentially incomplete pair near question: '{question[:50]}...'\")\n",
    "\n",
    "  except Exception as e:\n",
    "      print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "file_path = \"algebra__sequence_next_term.txt\"\n",
    "math_dataset = Dataset.from_generator(\n",
    "    custom_generator,\n",
    "    gen_kwargs={\"filepath\":file_path}\n",
    ")\n",
    "print(f\"Loaded {len(math_dataset)} total examples.\")\n",
    "math_dataset.to_json(\"train.jsonl\")\n",
    "dataset_path = \"train.jsonl\"\n",
    "problems = []\n",
    "with jsonlines.open(dataset_path) as reader:\n",
    "    for obj in reader:\n",
    "        problems.append({\"question\":obj[\"question\"],\"answer\":obj[\"answer\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "348cfbcc-5af8-473b-ab16-fbd27663123a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the next term in -890, -28, 782, 1534, 2222?', 'answer': '2840'}\n"
     ]
    }
   ],
   "source": [
    "print(problems[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d49633a-bc72-4625-8bd1-1d52239ab55d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Policy es la clase que usamos para representar a nuestro lm como la red que maximiza la recompensa, al cambiar sus parametros\n",
    "\n",
    "#### * para seleccionar la acciÃ³n , hacemos un sampling desde la distribuciÃ³n de tokens\n",
    "#### * calculamos log pobability de ese token, esto nos permite tener un rango mÃ¡s grande y reduce costos computacionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b830d5b9-3bba-4363-a101-e1f3a7b45f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    #gamma es el hiperparametro que usamos psra descontar recompensas futuras \n",
    "    def __init__(self,pretrained_model,gamma = 0.99):\n",
    "        super(Policy,self).__init__()\n",
    "        self.model = pretrained_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.gamma = gamma\n",
    "        # guardamos los log probabilities de cada accion\n",
    "        self.policy_history = []\n",
    "        #guardamos las recompensas de cada episodio\n",
    "        self.reward_episode = []\n",
    "    def forward(self,input_ids):\n",
    "        output = self.model(input_ids)\n",
    "        logits = outputs.logits([:,-1,:])\n",
    "        probs = nn.Softmax(dim=-1)(logits)\n",
    "        #retorna un tensor del tamano vocab size, con las probabilidades, pasado por un sofmax\n",
    "        return probs\n",
    "    def select_action(policy,input_ids):\n",
    "        probs = policy(input_ids)\n",
    "        #convertimos a los tokens en una distribucion y hacemos sampling\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample()\n",
    "        #calculamos log_probs\n",
    "        log_prob = dist.log_prob(action)\n",
    "        policy.policy_history.append(log_prob)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094a289-cc22-4f76-ae38-a8508bfa6fcc",
   "metadata": {},
   "source": [
    "### Ajustando los parÃ¡metros del modelo\n",
    "#### * damos prioridad a recompensas mas cercanas \n",
    "#### * para clacular loss: \n",
    "#### - asignamos recompensas a cada token, todos son cero excepto el ultimo (la respuesta)\n",
    "#### - ajustamos las recompensas a discounted_rewards[] usando gamma. esto da a cada token generado \"credito\" por la respuesta\n",
    "#### - loss se calcula como -= log_prob *  recompensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f46f23a-f730-4eeb-ae43-a8c7d0202052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(policy,optimizer):\n",
    "    discounted_rewards = []\n",
    "    running_reward = 0\n",
    "    #recorrer la lista desde el final\n",
    "    for r in policy.reward_episode[::-1]:\n",
    "        running_reward = r + policy.gamma * running_reward\n",
    "        discounted_rewards.insert(0,running_reward)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards,dtype=torch.float32).to(device)\n",
    "    if discounted_rewards.std() > 0:\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
    "    loss = 0\n",
    "    #multiplicamos la probabilidad del token por su recompensa\n",
    "    for log_prob,reward in zip(policy.policy_history,discounted_rewards):\n",
    "        loss -= log_prob * reward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    policy.policy_history = []\n",
    "    policy.reward_episode = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4258b-cf85-4cf2-8d51-99f672df12bc",
   "metadata": {},
   "source": [
    "### Por que se usa el negativo en loss?\n",
    "#### Log probabilities son negativas, la multiplicacion da una respuesta negativa y necesitamos aumentar la probabilidad de buenas acciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f1d7a-5205-42be-9e53-64945399e623",
   "metadata": {},
   "source": [
    "#### tratamos al modelo como un policy network, que genera acciones(tokens)\n",
    "#### Estado: el prompt\n",
    "#### Accion: cada unpo de los tokens generados \n",
    "#### Recompensa: 1 si la respuesta es correcta, 0.5 por un buen razonamiento, 0 de otro modo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f372dc1-6a5a-44bd-bdd7-67fcf69186fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    #gamma es el hiperparametro que usamos psra descontar recompensas futuras \n",
    "    def __init__(self,pretrained_model,gamma = 0.99):\n",
    "        super(Policy,self).__init__()\n",
    "        self.model = pretrained_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.gamma = gamma\n",
    "        # guardamos los log probabilities de cada accion\n",
    "        self.policy_history = []\n",
    "        #guardamos las recompensas de cada episodio\n",
    "        self.reward_episode = []\n",
    "    def forward(self,input_ids):\n",
    "        output = self.model(input_ids)\n",
    "        logits = outputs.logits[:,-1,:]\n",
    "        probs = nn.Softmax(dim=-1)(logits)\n",
    "        #retorna un tensor del tamano vocab size, con las probabilidades, pasado por un sofmax\n",
    "        return probs\n",
    "    def select_action(self,policy,input_ids):\n",
    "        probs = self.forward(input_ids)\n",
    "        #convertimos a los tokens en una distribucion y hacemos sampling\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample()\n",
    "        #calculamos log_probs\n",
    "        log_prob = dist.log_prob(action)\n",
    "        policy.policy_history.append(log_prob)\n",
    "        return action\n",
    "policy = Policy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee871063-a9df-44ef-ad57-e0489d2f6fe9",
   "metadata": {},
   "source": [
    "### Entrenamiento:\n",
    "#### usamos Adam para actualizar parametros de gemma-2b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fdc0dae-88e6-4c8d-868f-f6f3e736eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numbers(text):\n",
    "    return[float(x) for x in re.findall(r'-?\\d+\\.?\\d*', text)]\n",
    "def calculate_reward(generated_text, gold_answer,prompt):\n",
    "    reward = 0.0\n",
    "    numbers = extract_numbers(generated_text)\n",
    "    sequence = extract_numbers(prompt)\n",
    "    gold_num = float(gold_answer)\n",
    "\n",
    "    first_diffs = [sequence[i+1] - sequence[i] for i in range(len(sequence)-1)]\n",
    "    if any(diff in numbers for diff in first_diffs):\n",
    "        reward += 0.2  \n",
    "    second_diffs = [first_diffs[i+1] - first_diffs[i] for i in range(len(first_diffs)-1)]\n",
    "    if any(diff in numbers for diff in second_diffs):\n",
    "        reward += 0.2  \n",
    "    next_first_diff = first_diffs[-1] + second_diffs[-1]\n",
    "    if next_first_diff in numbers:\n",
    "        reward += 0.2  \n",
    "    if gold_num in numbers:\n",
    "        reward += 1.0\n",
    "\n",
    "    generated_embedding = model_embedding.encode(generated_text,convert_to_tensor=True)\n",
    "    similarity = torch.cosine_similarity(reference_embedding,generated_embedding,dim=0).item()\n",
    "    reward += similarity\n",
    "    return reward\n",
    "    \n",
    "\n",
    "    \n",
    "    # Assume generated_text ends with an expression\n",
    "    generated_answer = generated_text.split(\"The answer is\")[-1].strip() if \"The answer is\" in generated_text else generated_text\n",
    "    gold = parse(gold_answer)\n",
    "    answer = parse(generated_answer)\n",
    "    return 1.0 if verify(gold, answer) else 0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96ec5838-f166-4100-88c2-e6a513bdce35",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Gold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgold_answer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m         update_policy(policy,optimizer)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrain_reasoning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m, in \u001b[0;36mtrain_reasoning\u001b[0;34m(policy, episodes, max_length)\u001b[0m\n\u001b[1;32m      8\u001b[0m generated \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_length):\n\u001b[0;32m---> 11\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m##$\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     generated \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([generated,action\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m     policy\u001b[38;5;241m.\u001b[39mreward_episode\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m, in \u001b[0;36mPolicy.select_action\u001b[0;34m(self, policy, input_ids)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m,policy,input_ids):\n\u001b[0;32m---> 19\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#convertimos a los tokens en una distribucion y hacemos sampling\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(probs\u001b[38;5;241m=\u001b[39mprobs)\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36mPolicy.forward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,input_ids):\n\u001b[1;32m     13\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(input_ids)\n\u001b[0;32m---> 14\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241m.\u001b[39mlogits[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\n\u001b[1;32m     15\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)(logits)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#retorna un tensor del tamano vocab size, con las probabilidades, pasado por un sofmax\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "def train_reasoning(policy, episodes=20, max_length=10):  # Reduced max_length\n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr=0.0001)\n",
    "    for episode in range(episodes):\n",
    "        problem = problems[episode % len(problems)]\n",
    "        prompt = problem[\"question\"]\n",
    "        gold_answer = problem[\"answer\"]\n",
    "        \n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=50).input_ids.to(device)\n",
    "        generated = input_ids.clone()\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            action = policy.select_action(generated)\n",
    "            generated = torch.cat([generated, action.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "            policy.reward_episode.append(0)\n",
    "        \n",
    "        output_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        reward = calculate_reward(output_text, gold_answer)\n",
    "        policy.reward_episode[-1] = reward\n",
    "        \n",
    "        print(f\"Episode {episode}: Question: {prompt}\")\n",
    "        print(f\"Generated: {output_text}, Gold: {gold_answer}, Reward: {reward}\")\n",
    "        \n",
    "        update_policy(policy, optimizer)\n",
    "        \n",
    "\n",
    "train_reasoning(policy,episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c03a1e5-9355-4fb2-8b26-756292f973b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
